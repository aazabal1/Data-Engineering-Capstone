{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note: This is a continuation of the Capstone Project. The first part of the projects (steps 1 and 2) have been done in the Capstone_Project_Steps_1_2.ipynb Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The database will consist of four dimension tables and one fact table. Starting with the fact table (fact_inmig), the data model is as follows:\n",
    "##### fact_inmig:\n",
    "\n",
    "* CID: double PRIMARY KEY\n",
    "* citizenship_country: string \n",
    "* residence_country: string \n",
    "* arrival_airport: string \n",
    "* arrival_date: date \n",
    "* departure_date: date \n",
    "* age: double \n",
    "* generic_visa_type: string \n",
    "* arrival_flag: string \n",
    "* departure_flag: string \n",
    "* gender: string \n",
    "* airline: string\n",
    "* arrival_flight_number: string \n",
    "* visa_type: string \n",
    "* arrival_state: string\n",
    "\n",
    "The dimension tables are:\n",
    "\n",
    "##### dim_airports:\n",
    "\n",
    "* ident: string PRIMARY KEY\n",
    "* type: string \n",
    "* elevation_ft: string \n",
    "* municipality: string\n",
    "* iata_code: string\n",
    "* State: string \n",
    "\n",
    "##### dim_demographics:\n",
    "\n",
    "* City_State: string PRIMARY KEY\n",
    "* City: string\n",
    "* Median_Age: string\n",
    "* Male_Population: string\n",
    "* Female_Population: string \n",
    "* Total_Population: string \n",
    "* Veterans: string \n",
    "* Foreign: string \n",
    "* Avg_House_Size: string\n",
    "* State: string \n",
    "\n",
    "##### dim_temperature:\n",
    "\n",
    "* dt: string \n",
    "* AverageTemperature: string\n",
    "* AverageTemperatureUncertainty: string\n",
    "* Latitude: string\n",
    "* Longitude: string\n",
    "* year: integer\n",
    "* City: string \n",
    "* PRIMARY KEY (dt, Latitude, Longitude)\n",
    "\n",
    "##### dim_time:\n",
    "\n",
    "* date: date PRIMARY KEY\n",
    "* day: integer \n",
    "* week: integer \n",
    "* weekday: integer\n",
    "* year_day: integer \n",
    "* year: integer \n",
    "* month: integer \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The process has already been partially completed in Section 2, but will be summarised here.\n",
    "\n",
    "#### Data Extraction:\n",
    "\n",
    "Load all the datasets from CSV and SAS data files;\n",
    "\n",
    "#### Data Transformation and Loading:\n",
    "\n",
    "##### fact_immigration:\n",
    "* Drop unwanted columns\n",
    "* Convert citizenship country from code to name\n",
    "* Convert residence country from code to name\n",
    "* Convert arrival date to proper date format\n",
    "* Keep only US addresses\n",
    "* Convert departure date to proper date format and keep rows where this is after the arrival date only\n",
    "* Convert visa ID to visa type description\n",
    "* Select only arrivals to airports\n",
    "* Select only desired columns\n",
    "* Save as parquet files \n",
    "\n",
    "##### dim_temperature:\n",
    "* Select data from 2000 onwards\n",
    "* Select only USA cities\n",
    "* Remove all null data points\n",
    "* Select only desired columns\n",
    "* Save as parquet files \n",
    "\n",
    "##### dim_time:\n",
    "* Get distinct values from the fact table to act as primary keys for time dimension data\n",
    "* Get time data and proper format\n",
    "* Select only desired columns\n",
    "* Save as parquet files \n",
    "\n",
    "##### dim_airports:\n",
    "* Select USA airports\n",
    "* Get airport USA state\n",
    "* Select only airports\n",
    "* Remove Null IATA codes\n",
    "* Prepare proper format and naming\n",
    "* Select only desired columns\n",
    "* Save as parquet files \n",
    "\n",
    "##### dim_city_demographics:\n",
    "* Remove unwanted rows and change names\n",
    "* Join city and state to generate unique column to act as primary key\n",
    "* Select only desired columns\n",
    "* Save as parquet files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Temperature Dimension Data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temp_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(fname)\n",
    "temp_df.createOrReplaceTempView(\"dim_temperature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# City Demographics Dimension Data\n",
    "demo_df =spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load('us-cities-demographics.csv')\n",
    "demo_df = demo_df.na.drop(subset=demo_df.columns)\n",
    "demo_df.createOrReplaceTempView(\"dim_demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Airport Dimension Data\n",
    "airports_df =spark.read.format(\"csv\").option(\"header\", \"true\").load('airport-codes_csv.csv')\n",
    "airports_df.createOrReplaceTempView(\"dim_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Inmigration Fact Data\n",
    "inmig_df=spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "inmig_df.createOrReplaceTempView(\"fact_inmig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='582', Country='MEXICO Air Sea, and Not Reported (I-94, no land arrivals)', Index='582')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Country Codes required for Inmigration Data transformation\n",
    "country_codes_df =spark.read.format(\"csv\").option(\"header\", \"true\").load('country_codes.csv')\n",
    "country_codes_df.createOrReplaceTempView(\"country_codes\")\n",
    "country_codes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select only data from 2000 onwards\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, year(dt) as year\n",
    "FROM dim_temperature\n",
    "WHERE year(dt) >=2000\n",
    "\"\"\").createOrReplaceTempView(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select cities from USA\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_temperature\n",
    "WHERE Country = 'United States'\n",
    "\"\"\").createOrReplaceTempView(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove all Null datapoints\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_temperature\n",
    "WHERE AverageTemperature IS NOT NULL\n",
    "AND AverageTemperatureUncertainty IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### City Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove unwanted rows and change names\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT lower(City) as City, \n",
    "        `Median Age` as Median_Age, \n",
    "        `Male Population` as Male_Population, \n",
    "        `Female Population` as Female_Population, \n",
    "        `Total Population` as Total_Population, \n",
    "        `Number of Veterans` as Veterans,\n",
    "        `Foreign-born` as Foreign,\n",
    "        `Average Household Size` as Avg_House_Size,\n",
    "        `State Code` as State\n",
    "FROM dim_demographics\n",
    "\"\"\").createOrReplaceTempView(\"dim_demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join city and state to generate unique column to act as primary key\n",
    "spark.sql(\"\"\"\n",
    "SELECT *,concat(City,\"_\",State) as City_State\n",
    "FROM dim_demographics\n",
    "\"\"\").createOrReplaceTempView(\"dim_demographics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select USA airports\n",
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM dim_airports\n",
    "WHERE iso_country='US'\n",
    "\"\"\").createOrReplaceTempView(\"dim_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get airport USA state\n",
    "spark.sql(\"\"\"\n",
    "SELECT *,RIGHT(iso_region, 2) AS Airport_State\n",
    "FROM dim_airports\n",
    "\"\"\").createOrReplaceTempView(\"dim_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select only airports\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_airports\n",
    "WHERE type IN ('small_airport','medium_airport','large_airport')\n",
    "\"\"\").createOrReplaceTempView(\"dim_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove Null IATA codes\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_airports\n",
    "WHERE iata_code IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView(\"dim_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare proper format and naming\n",
    "spark.sql(\"\"\"\n",
    "SELECT ident, type, elevation_ft, Airport_State as State, lower(municipality) as municipality, iata_code\n",
    "FROM dim_airports\n",
    "\"\"\").createOrReplaceTempView(\"dim_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove null elevations\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_airports\n",
    "WHERE elevation_ft IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView(\"dim_airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Inmigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "\n",
    "inmig_df = inmig_df.drop(\"visapost\",\"occup\",\"entdepu\",\"insnum\",\"count\",\"dtadfile\",\"dtaddto\",\"admnum\",\"matflag\",\"biryear\")\n",
    "inmig_df = inmig_df.na.drop(subset=inmig_df.columns)\n",
    "inmig_df.createOrReplaceTempView(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert citizenship country from code to name\n",
    "spark.sql(\"SELECT *, lower(country_codes.Country) as citizenship_country\\\n",
    "          FROM fact_inmig \\\n",
    "          INNER JOIN country_codes \\\n",
    "          ON fact_inmig.i94cit = country_codes.Index \\\n",
    "         \").createOrReplaceTempView(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert residence country from code to name\n",
    "\n",
    "spark.sql(\"SELECT *, lower(country_codes.Country) as residence_country\\\n",
    "          FROM fact_inmig \\\n",
    "          INNER JOIN country_codes \\\n",
    "          ON fact_inmig.i94res = country_codes.Index \\\n",
    "         \").createOrReplaceTempView(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert arrival date to proper date format\n",
    "spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date\\\n",
    "          FROM fact_inmig\"\n",
    "         ).createOrReplaceTempView(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep only US addresses\n",
    "\n",
    "spark.sql(\"SELECT *\\\n",
    "          FROM fact_inmig\\\n",
    "          WHERE i94addr <> '99'\").createOrReplaceTempView(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert departure date to proper date format and keep rows where this is after the arrival date only\n",
    "spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), depdate) AS departure_date\\\n",
    "          FROM fact_inmig\\\n",
    "          WHERE date_add(to_date('1960-01-01'), depdate)>=arrival_date\"\n",
    "         ).createOrReplaceTempView(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert visa ID to visa type description\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN i94visa = 1.0 THEN 'Business' \n",
    "                        WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                        WHEN i94visa = 3.0 THEN 'Student'\n",
    "                        ELSE '0' END AS generic_visa_type \n",
    "                FROM fact_inmig\"\"\").createOrReplaceTempView(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select only arrivals to airports\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM fact_inmig\n",
    "WHERE i94mode = 1\n",
    "\"\"\").createOrReplaceTempView(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Time Dimension Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get distinct values from the fact table to act as primary keys for time dimension data\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT DISTINCT arrival_date AS date\n",
    "FROM fact_inmig\n",
    "\"\"\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get time data and proper format\n",
    "spark.sql(\"\"\"\n",
    "SELECT date, YEAR(date) AS year, MONTH(date) AS month, DAY(date) AS day, WEEKOFYEAR(date) AS week, DAYOFWEEK(date) as weekday, DAYOFYEAR(date) year_day\n",
    "FROM dim_time\n",
    "ORDER BY date ASC\n",
    "\"\"\").createOrReplaceTempView(\"dim_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select only desired temperature columns\n",
    "dim_temperature = spark.sql(\"\"\"\n",
    "SELECT dt, AverageTemperature,AverageTemperatureUncertainty, lower(City) as City, Latitude, Longitude, year\n",
    "FROM dim_temperature\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save as parquet files\n",
    "dim_temperature.write.partitionBy(\"year\",\"City\").parquet(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### City Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select all columns\n",
    "dim_demographics = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_demographics\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demographics = dim_demographics.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demographics.write.partitionBy(\"State\").parquet(\"dim_demographics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select all columns\n",
    "dim_airports = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_airports\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airports.write.partitionBy(\"State\").parquet(\"dim_airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Inmigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select only desired columns\n",
    "fact_inmig = spark.sql(\"\"\"\n",
    "SELECT cicid as CID,\n",
    "        citizenship_country,\n",
    "        residence_country,\n",
    "        i94port as arrival_airport,\n",
    "        arrival_date,\n",
    "        i94addr as arrival_state,\n",
    "        departure_date,\n",
    "        i94bir as age,\n",
    "        generic_visa_type,\n",
    "        entdepa as arrival_flag,\n",
    "        entdepd as departure_flag,\n",
    "        gender,\n",
    "        airline,\n",
    "        fltno as arrival_flight_number,\n",
    "        visatype as visa_type\n",
    "FROM fact_inmig\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_inmig.write.partitionBy(\"arrival_date\").parquet(\"fact_inmig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Time Dimension Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select only desired columns\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_time\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_time.write.partitionBy(\"year\",\"month\").parquet(\"dim_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------------+---------------+-------------+--------------+----+-----------------+------------+--------------+------+-------+---------------------+---------+------------+\n",
      "|      CID|citizenship_country|residence_country|arrival_airport|arrival_state|departure_date| age|generic_visa_type|arrival_flag|departure_flag|gender|airline|arrival_flight_number|visa_type|arrival_date|\n",
      "+---------+-------------------+-----------------+---------------+-------------+--------------+----+-----------------+------------+--------------+------+-------+---------------------+---------+------------+\n",
      "|5417720.0|            albania|          albania|            CHI|           MI|    2016-05-28|31.0|         Pleasure|           T|             O|     M|     OS|                   65|       B2|  2016-04-29|\n",
      "|5417721.0|            albania|          albania|            CLG|           TX|    2016-05-01|20.0|         Pleasure|           H|             O|     F|     QK|                08104|       B2|  2016-04-29|\n",
      "+---------+-------------------+-----------------+---------------+-------------+--------------+----+-----------------+------------+--------------+------+-------+---------------------+---------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First check that data looks like it should\n",
    "ETL_inmigration = spark.read.parquet(\"fact_inmig\")\n",
    "ETL_inmigration.createOrReplaceTempView(\"check_inm\")\n",
    "ETL_inmigration.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------------+-------------+---------+-----+\n",
      "|ident|         type|elevation_ft| municipality|iata_code|State|\n",
      "+-----+-------------+------------+-------------+---------+-----+\n",
      "|  0AK|small_airport|         305|pilot station|      PQS|   AK|\n",
      "|  16A|small_airport|          12|  nunapitchuk|      NUP|   AK|\n",
      "+-----+-------------+------------+-------------+---------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First check that data looks like it should\n",
    "ETL_airports = spark.read.parquet(\"dim_airports\")\n",
    "ETL_airports.createOrReplaceTempView(\"check_airports\")\n",
    "ETL_airports.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------------+-----------------+----------------+--------+-------+--------------+--------------------+-----+\n",
      "|                City|Median_Age|Male_Population|Female_Population|Total_Population|Veterans|Foreign|Avg_House_Size|          City_State|State|\n",
      "+--------------------+----------+---------------+-----------------+----------------+--------+-------+--------------+--------------------+-----+\n",
      "|louisville/jeffer...|      37.5|         298451|           316938|          615389|   39364|  37875|          2.45|louisville/jeffer...|   KY|\n",
      "|athens-clarke cou...|      26.5|          57415|            65148|          122563|    3953|  12868|          2.44|athens-clarke cou...|   GA|\n",
      "+--------------------+----------+---------------+-----------------+----------------+--------+-------+--------------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First check that data looks like it should\n",
    "ETL_demographics = spark.read.parquet(\"dim_demographics\")\n",
    "ETL_demographics.createOrReplaceTempView(\"check_demographics\")\n",
    "\n",
    "ETL_demographics.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------------------+--------+---------+----+-----------+\n",
      "|        dt|  AverageTemperature|AverageTemperatureUncertainty|Latitude|Longitude|year|       City|\n",
      "+----------+--------------------+-----------------------------+--------+---------+----+-----------+\n",
      "|2007-01-01|-0.01499999999999...|                        0.295|  37.78N|   93.56W|2007|springfield|\n",
      "|2007-02-01|-0.14900000000000002|                        0.466|  37.78N|   93.56W|2007|springfield|\n",
      "+----------+--------------------+-----------------------------+--------+---------+----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First check that data looks like it should\n",
    "ETL_temperature = spark.read.parquet(\"dim_temperature\")\n",
    "ETL_temperature.createOrReplaceTempView(\"check_temperature\")\n",
    "\n",
    "ETL_temperature.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-------+--------+----+-----+\n",
      "|      date|day|week|weekday|year_day|year|month|\n",
      "+----------+---+----+-------+--------+----+-----+\n",
      "|2016-04-10| 10|  14|      1|     101|2016|    4|\n",
      "|2016-04-15| 15|  15|      6|     106|2016|    4|\n",
      "+----------+---+----+-------+--------+----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First check that data looks like it should\n",
    "ETL_time = spark.read.parquet(\"dim_time\")\n",
    "ETL_time.createOrReplaceTempView(\"check_time\")\n",
    "\n",
    "ETL_time.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|count(DISTINCT CID)|count(1)|\n",
      "+-------------------+--------+\n",
      "|            2087669| 2087669|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "# Check unique keys\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT(CID)), COUNT(*)\n",
    "FROM check_inm\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These are the same, which means that the primary key is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|count(DISTINCT ident)|count(1)|\n",
      "+---------------------+--------+\n",
      "|                 1864|    1864|\n",
      "+---------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check unique keys\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT(ident)), COUNT(*)\n",
    "FROM check_airports\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These are the same, which means that the primary key is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------+\n",
      "|count(DISTINCT City_State)|count(1)|\n",
      "+--------------------------+--------+\n",
      "|                       588|     588|\n",
      "+--------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check unique keys\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT(City_State)), COUNT(*)\n",
    "FROM check_demographics\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These are the same, which means that the primary key is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|count(DISTINCT date)|count(1)|\n",
      "+--------------------+--------+\n",
      "|                  30|      30|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check unique keys\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT(date)), COUNT(*)\n",
    "FROM check_time\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These are the same, which means that the primary key is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_null(table, column):\n",
    "    \"\"\" Check if any NULL value in the given table for the given column\n",
    "        Args: table - table to analyse\n",
    "              column - column from table to analyse\n",
    "    \"\"\"\n",
    "    check_val = spark.sql(f\"\"\"SELECT COUNT(*) as nbr FROM {table} WHERE {column} IS NULL\"\"\").head()[0]\n",
    "    if check_val >0:\n",
    "        raise ValueError(f\"Error in data quality. Found{check_val} NULL values in {column} column\")\n",
    "    else:\n",
    "        print(f\"Correct data quality for {table} in column {column}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Fact Inmigration table\n",
      "Correct data quality for check_inm in column CID.\n",
      "Correct data quality for check_inm in column citizenship_country.\n",
      "Correct data quality for check_inm in column residence_country.\n",
      "Correct data quality for check_inm in column arrival_airport.\n",
      "Correct data quality for check_inm in column arrival_state.\n",
      "Correct data quality for check_inm in column departure_date.\n",
      "Correct data quality for check_inm in column age.\n",
      "Correct data quality for check_inm in column generic_visa_type.\n",
      "Correct data quality for check_inm in column arrival_flag.\n",
      "Correct data quality for check_inm in column departure_flag.\n",
      "Correct data quality for check_inm in column gender.\n",
      "Correct data quality for check_inm in column airline.\n",
      "Correct data quality for check_inm in column arrival_flight_number.\n",
      "Correct data quality for check_inm in column visa_type.\n",
      "Correct data quality for check_inm in column arrival_date.\n",
      " \n",
      "Checking Dimension Airport table\n",
      "Correct data quality for check_airports in column ident.\n",
      "Correct data quality for check_airports in column type.\n",
      "Correct data quality for check_airports in column elevation_ft.\n",
      "Correct data quality for check_airports in column municipality.\n",
      "Correct data quality for check_airports in column iata_code.\n",
      "Correct data quality for check_airports in column State.\n",
      " \n",
      "Checking Dimension Demographics table\n",
      "Correct data quality for check_demographics in column City.\n",
      "Correct data quality for check_demographics in column Median_Age.\n",
      "Correct data quality for check_demographics in column Male_Population.\n",
      "Correct data quality for check_demographics in column Female_Population.\n",
      "Correct data quality for check_demographics in column Total_Population.\n",
      "Correct data quality for check_demographics in column Veterans.\n",
      "Correct data quality for check_demographics in column Foreign.\n",
      "Correct data quality for check_demographics in column Avg_House_Size.\n",
      "Correct data quality for check_demographics in column City_State.\n",
      "Correct data quality for check_demographics in column State.\n",
      " \n",
      "Checking Dimension Temperature table\n",
      "Correct data quality for check_temperature in column dt.\n",
      "Correct data quality for check_temperature in column AverageTemperature.\n",
      "Correct data quality for check_temperature in column AverageTemperatureUncertainty.\n",
      "Correct data quality for check_temperature in column Latitude.\n",
      "Correct data quality for check_temperature in column Longitude.\n",
      "Correct data quality for check_temperature in column year.\n",
      "Correct data quality for check_temperature in column City.\n",
      " \n",
      "Checking Dimension Time table\n",
      "Correct data quality for check_time in column date.\n",
      "Correct data quality for check_time in column day.\n",
      "Correct data quality for check_time in column week.\n",
      "Correct data quality for check_time in column weekday.\n",
      "Correct data quality for check_time in column year_day.\n",
      "Correct data quality for check_time in column year.\n",
      "Correct data quality for check_time in column month.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Checking Fact Inmigration table\")\n",
    "for column in ETL_inmigration.columns:\n",
    "    check_null(\"check_inm\", column)\n",
    "print(\" \")\n",
    "print(\"Checking Dimension Airport table\")\n",
    "for column in ETL_airports.columns:\n",
    "    check_null(\"check_airports\", column)\n",
    "print(\" \")\n",
    "print(\"Checking Dimension Demographics table\")\n",
    "for column in ETL_demographics.columns:\n",
    "    check_null(\"check_demographics\", column)\n",
    "print(\" \")\n",
    "print(\"Checking Dimension Temperature table\")\n",
    "for column in ETL_temperature.columns:\n",
    "    check_null(\"check_temperature\", column)\n",
    "print(\" \")\n",
    "print(\"Checking Dimension Time table\")\n",
    "for column in ETL_time.columns:\n",
    "    check_null(\"check_time\", column)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "    The decision to work locally using Spark rather than on AWS, Airflow or other distributed tools was based on the fact that although the data is large (more than 3 million rows) it is small enough that it fits in the memory of the local machine I'm using. On top of this, this is a project that doesn't require collaboration from other teammates nor constant data updating, as this is just a demonsrator. Thus is it valid enough to work locally. \n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "    If this dataset were to be properly used by other people throughout time, it would have to be constantly updated. The latency of the data will vary depending on the end use of the data and the computational power, varying from hourly if the data were to be used by the police and Border Control to analyse airport occupation, to daily or weekly if used for other purposes, such as a more generic analysis of the inflow of passengers into the USA to assess demographic changes. Personally I would suggest updating this data weekly, as it is the proper balance between computational power and size of data that nees to be updated (several thousand inmigrants come into the USA every week).\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    "    In this case, the local machine I'm using would run out of memory, so I would need to transfer the data to a cloud warehouse (Amazon's S3 is a good example), and utilising several nodes in these buckets. Having the data stored in S3 the ETL pipeline could be prepared using Redshift instead, but still using Spark and SQL as the languages, as these are the best suited for big data analysis.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    "     The ETL pipeline could be migrated into Apache Airflow, which allows for automated updated and proper data quality checks. Once in Airflow, the ETL pipeline would have to be converted into DAGs, that follow the same idea and meet the same purposes.\n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    "     Once the ETL pipeline is done the data can be stored in a Redshift cluster, which can be accessed by anyone who had the proper access to the cluster (key and password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
